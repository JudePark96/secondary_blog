---
layout: post
title: "RNN 을 이용한 단어 예측하기"
date: 2019-04-22
tags: [RNN, keras]
excerpt: "RNN 을 이용하여 주어진 단어 다음에 올 단어 예측해보자"
comments: true
---

### 1. Introduction

RNN을 통해서 주어진 문장을 학습하고, 주어진 단어 다음에 올 단어를 예측해본다.

### 2. Code

```python3
text = "나랑 점심 먹으러 갈래 메뉴는 햄버거 점심 메뉴 좋지"
```
위와 같이 주어진 문장을 one-hot vector 로 만들고 학습한다.

	import numpy as np
	from keras import preprocessing
	from keras.utils import to_categorical
	from keras.layers import Embedding, Dense, SimpleRNN, LSTM, GRU
	from keras.models import Sequential
우선 필요한 패키지를 불러온다.

```python
from keras_preprocessing.text import Tokenizer

t = Tokenizer()
t.fit_on_texts([text])
encoded = t.texts_to_sequences([text])[0]

vocab_size = len(t.word_index) + 1
```
encoded 와 t.word_index 를 출력해보면 다음과 같이 나온다.

```
([2, 1, 3, 4, 5, 6, 1, 7, 8],
 {'갈래': 4, '나랑': 2, '먹으러': 3, '메뉴': 7, '메뉴는': 5, '점심': 1, '좋지': 8, '햄버거': 6})
```

t.word_index 의 index 는 각 단어의 빈도 수에 따라 부여되었다. 점심은 2번 등장하였기 때문에 1-idx 를 부여받았다.

```python
seqs = []
for c in range(1, len(encoded)):
    seq = encoded[c-1:c+1]
    seqs.append(seq)
    
print('count of word collection: %d' % len(seqs))
```
문장에서 단어를 묶는 작업을 한다. `[(나랑, 점심), (점심, 먹으러) ...]` 꼴이며 실제 단어 대신 **one-hot vector** 가 들어간다. Bi-gram 과 같다고 보면 된다.

> <https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/>

```python
X, y = zip(*seqs)
X = np.array(X)
y = np.array(y)

y = to_categorical(y, num_classes=vocab_size) # one-hot encoding
```
훈련시킬 데이터 X 와 결과 값 y 를 이제 학습시켜본다.

```python
model = Sequential()
model.add(Embedding(vocab_size, 9, input_length=1))
model.add(SimpleRNN(9))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=500, verbose=1)
```
위의 코드를 실행하면 예측 모델이 만들어진다.

### 3. References

<https://wikidocs.net/22886>

<https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/>

